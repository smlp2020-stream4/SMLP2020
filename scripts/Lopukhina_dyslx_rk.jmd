---
title: "Reading children"
author: "Anastasiya Lopukhina"
date: "2020-09-09"
---

**rk comments**

# Setup

Packages we (might) use.

```julia

using DrWatson
@quickactivate "SMLP2020"

using MixedModels, Serialization
using DataFrames, DataFramesMeta, RCall 
using Statistics: mean
```

```julia
R"""
require("lme4", quietly=TRUE)
require("remef")
require("tidyverse", quietly=TRUE)
require("broom.mixed", quietly=TRUE)
require("performance", quietly=TRUE)
require("grid", quietly=TRUE)
require("gridExtra", quietly=TRUE)
source($srcdir('LMM_residuals.R'))
""";

RCall.ijulia_setdevice(MIME("image/svg+xml"), width=6, height=3.5)
```

We read the data preprocessed with R and saved as RDS file (see `Lopukhina_dyslx.Rmd`).

```julia
dat = DataFrame(rcopy(R"readRDS($datadir('Lopukhina_ffd.RDS'))"));
describe(dat)
dat.lp2 = abs2.(dat.lp);
dat.ls = dat.ao1 - dat.lp
```

**DV**
log(ffd):  log of first fixation duration

**Predictors**

+ f: log frequency of word
+ l: length of word
+ grd: grade level of child
+ snd: sounding skill of child

**Control**

+ ao1: amplitude (incoming) - launch site?  
+ lp:  landing position (IOVP)  dat.lp2 = abs2.(dat.lp);
+ lp2 = abs2.(lp)
+ l1:  length of last word
+ l2:  length of next word
+ rpw: relative position of word in sentence

**Group**

Non-dyslexic children serve as control group for dyslexic children (treamtent/dummy contrast).

```julia
dat = @transform(dat, Group = levels!(:Group, ["norm", "dyslexia"]))
```

## LMM with lme4

AL's `oviLMM`

```julia
@rput dat;

R"""
ffd_m1 <- lmer(log(ffd) ~ 1 + f + l + grd + snd + 
                         lp + l1 + l2 + rpw +
                         (1 | Subj ) + (1 | Item) + (1 | Word), 
                data=dat,  REML = FALSE,
                control = lmerControl(calc.derivs=FALSE))

check_model(ffd_m1, check=c('qq', 'reqq'))
#plot_LMM_residuals(ffd_m1)

#plot(fitted(ffd_m1), residuals(ffd_m1))
#qqnorm(residuals(ffd_m1))
"""
```

Adding "Group", using "norm" as reference for "dyslexia"

```julia
R"""
contrasts(dat$Group) <- contr.treatment(2, base=2)
mm <- model.matrix( ~ 1 + Group, data=dat)
dat$grp <-  mm[,2]
table(dat$Group, dat$grp)

ffd_m2 <- lmer(log(ffd) ~ 1 + Group*(grd + snd + f + l + lp + lp2 + l1 + l2 + rpw) +
                         (1 | Subj ) + (1 | Item) + (1 | Word), 
                data = dat,  REML = FALSE,
                control = lmerControl(calc.derivs=FALSE))

anova(ffd_m1, ffd_m2)
"""
```

# LMM with `MixedModels`

## Contrasts

```julia
cntrsts = merge(
    Dict(:Group => DummyCoding()),
    Dict(index => Grouping() for index in (:Subj, :Word, :Item,)),
);
```

## AL's LMM

```julia
f1 = @formula (log(ffd))  ~ 1 + grd + snd + f + l + lp + lp2 + l1 + l2 + rpw +
                           (1 | Subj ) + (1 | Item) + (1 | Word);
m1 = fit(MixedModel, f1, dat, contrasts=cntrsts);
```

## Adding `Group` and its interaction with covariates

```julia
f2 = @formula (log(ffd))  ~ 1 + Group*(grd + snd + f + l + lp + lp2 + l1 + l2 + rpw) +
                           (1 | Subj ) + (1 | Item) + (1 | Word);
m2 = fit(MixedModel, f2, dat, contrasts=cntrsts);

MixedModels.likelihoodratiotest(m2, m1)
```

## Adding VPs

Checking individual and item differences in `l1`, `ao`, and  `rpw` -- also their interaction with `Group`. Now we need to engage in model selection. 

```julia
f3 = @formula (log(ffd))  ~ 1 + Group*(grd + snd + f + l + lp + lp2 + l1 + l2 + rpw)  + 
               zerocorr(1 + f + l + lp + lp2 + l1 + l2 + rpw  | Subj) + 
               zerocorr(1 + f + l + lp + lp2 + l1 + l2 + rpw  + grd + snd + Group | Item) + 
               zerocorr(1         + lp + lp2 + l1 + l2 + rpw  + grd + snd + Group | Word);
m3 = fit(MixedModel, f3, dat, contrasts=cntrsts);
m3.rePCA
VarCorr(m3)


# Save LMM
serialize(projectdir("fits", "m3.jls"), m3) 
# Retrieve LMM
deserialize(projectdir("fits", "m3.jls"));
```

There are a many very small VCs; the model is seriously overparameterized. I take out VCs estimated at zero. 

```julia
f4 = @formula (log(ffd))  ~ 1 + Group*(grd + snd + f + l + lp + lp2 + l1 + l2 + rpw)  + 
              zerocorr(1 + lp + lp2 + rpw                | Subj) + 
              zerocorr(1            + rpw  + grd + Group | Item) + 
              zerocorr(1 + lp + lp2 + rpw  + snd         | Word);
  
m4 = fit(MixedModel, f4, dat, contrasts=cntrsts);
m4.rePCA
VarCorr(m4)

MixedModels.likelihoodratiotest(m4, m3)

# save 
serialize(projectdir("fits", "m4.jls"), m4) 

```

LMM `m4` is supported by the data and does not fit worse than LMM `m3`. 
Let's add the CPs for these VPs.

```julia
f5 = @formula (log(ffd))  ~ 1 + Group*(grd + snd + f + l + lp + lp2 + l1 + l2 + rpw)  + 
               (1 + lp + lp2 + rpw                | Subj) + 
               (1            + rpw  + grd + Group | Item) + 
               (1 + lp + lp2 + rpw  + snd         | Word);
  
m5 = fit(MixedModel, f5, dat, contrasts=cntrsts);
m5.rePCA
VarCorr(m5)

MixedModels.likelihoodratiotest(m5, m4, m3)

# save
serialize(projectdir("fits", "m5.jls"), m5) 
```

Adding CPs significantly improves the goodness of fit, but it is overparameterized in the `Word` and the `Item` factor. 
Some of us think that an overparameterized model must not / should not (?) be used as a reference model in a LRT. 

So can we prune some more? 

```julia
f6 = @formula (log(ffd)) ~ 1 + Group*(grd + snd + f + l + lp + lp2 + l1 + l2 + rpw)  + 
               (1 + lp + lp2 + rpw  | Subj) + 
               zerocorr(1 + rpw + Group | Item) + 
               zerocorr(1 + lp + lp2 + rpw | Word);

m6 = fit(MixedModel, f6, dat, contrasts=cntrsts);
m6.rePCA
VarCorr(m6)
MixedModels.likelihoodratiotest( m5, m6)

mods = [m3, m4, m6, m5];
gof_summary = DataFrame(dof=dof.(mods), deviance=deviance.(mods),
              AIC = aic.(mods), AICc = aicc.(mods), BIC = bic.(mods))

# save
serialize(projectdir("fits", "m6.jls"), m6) 
```

This was clearly an exploratory setting. In this situation, I tend to use the most conservative information criterion, that is the BIC.  It suggests going from m5 to m6 reduces the risk of overfitting, plus m6 is overparameterized. I would stay with m6. 

What is significant in the fixed effects?

```julia
show(m6)
```

There significant effects of grade and landing position, but no interactions with group.

## Ship fitted model to R for postprocessing

```julia
m6b = Tuple([m6, dat]);
@rput m6b;
```

Well, not all of them yet ...

## Model diagnostics

```julia

```

# Appendix 

## Weave the document in the REPL

```
julia> using Weave
julia> weave(scriptsdir("Lopukhina_dyslx_rk.jmd"), doctype="md2html")
```

## Switch to jupyter notebook from REPL

```
julia> using Weave, IJulia
julia> convert_doc(scriptsdir("Lopukhina_dyslx_rk.jmd"), projectdir("notebooks","Lopukhina_dyslx_rk.ipynb"))
julia> IJulia.notebook(dir=projectdir("notebooks"))
```

## Info

```julia
using InteractiveUtils
versioninfo()
```
